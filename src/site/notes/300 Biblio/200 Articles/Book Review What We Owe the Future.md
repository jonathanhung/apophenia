---
{"type":"articles","date_created":"2022-08-26","aliases":null,"topic":null,"url":"https://astralcodexten.substack.com/p/book-review-what-we-owe-the-future","layout":null,"banner":null,"dg-publish":true,"tags":null,"permalink":"/300-biblio/200-articles/book-review-what-we-owe-the-future/","dgPassFrontmatter":true,"created":"2023-10-20T12:44:18.000-05:00","updated":"2023-10-20T12:44:18.000-05:00"}
---

## Metadata
---
![rw-book-cover](https://readwise-assets.s3.amazonaws.com/static/images/article3.5c705a01b476.png)
- Author:: [[astralcodexten.substack.com\|astralcodexten.substack.com]]
- Title:: Book Review: What We Owe the Future
- topic::  



## Insights
---
## Related Nodes
---

## Highlights 
---
- Is this just Pascalian reasoning, where you name a prize so big that it overwhelms any potential discussion of how likely it is that you can really get the prize? MacAskill carefully avoids doing this explicitly, so much so that he (unconvincingly) denies being a utilitarian at all. Is he doing it implicitly? I think he would make an argument something like Gregory Lewis’ Most Small Probabilities Aren’t Pascalian. This isn’t about an 0.000001% chance of affecting 50 quadrillion people. It’s more like a 1% chance of affecting them. It’s not automatically Pascalian reasoning every time you’re dealing with a high-stakes situation! ([View Highlight](https://instapaper.com/read/1532400937/20501530))
- You might think: “If I help Andorra, it will only benefit a few thousand people. If I help Lithuania, it will only benefit a few million people. But if I help India, it will benefit over a billion people. So I will devote my life to helping India.” Then you learn about the future, a country with 50 quadrillion people. Seems like a big deal ([View Highlight](https://instapaper.com/read/1532400937/20501533))
- When growth slows, everyone becomes fiercely protective of what they have, and play zero-sum games with each other in ways not conducive to future growth ([View Highlight](https://instapaper.com/read/1532400937/20501539))
- MacAskill frames this in terms of value malleability and value lock-in. There is a time of great malleability: maybe during the Constitutional Convention, if some delegate had given a slightly more elegant speech, they might have ditched the Senate or doubled the length of a presidential term or something. But after the Constitution was signed - and after it developed centuries of respect, and after tense battle lines got drawn up over every aspect of it - it became much harder to change the Constitution ([View Highlight](https://instapaper.com/read/1532400937/20501574))
- Objection: Mohammed, Washington, and Confucius shaped the future. But none of them could really see how their influence would ripple through time, and they might not be very happy with the civilizations they created. Do we have any examples of people who aimed for a certain positive change to the future, achieved it, and locked it in so hard that we expect it to continue even unto the ends of the galaxy? ([View Highlight](https://instapaper.com/read/1532400937/20501583))
- What is at the “swing a few court cases” stage today?
  MacAskill doesn’t talk about this much besides gesturing about something something AI. Instead, he focuses on ideas he calls “moral entrepreneurship” and “moral exploration”; can we do what Benjamin Lay did in the 1700s and discover moral truths we were missing before of the same scale as “slavery is wrong”? ([View Highlight](https://instapaper.com/read/1532400937/20501665))
- Most of me agrees with MacAskill’s boring good-PR point: long-termism rarely gives different answers from near-termism. ([View Highlight](https://instapaper.com/read/1532400937/20501711))
- If I had to play the philosophy game, I would assert that it’s always bad to create new people whose lives are below zero, and neutral to slightly bad to create new people whose lives are positive but below average. This sort of implies that very poor people shouldn’t have kids, but I’m happy to shrug this off by saying it’s a very minor sin and the joy that the child brings the parents more than compensates for the harm against abstract utility. This series of commitments feels basically right to me and I think it prevents muggings.
  But I’m not sure I want to play the philosophy game. Maybe MacAskill can come up with some clever proof that the commitments I list above imply I have to have my eyes pecked out by angry seagulls or something. If that’s true, I will just not do that, and switch to some other set of axioms. If I can’t find any system of axioms that doesn’t do something terrible when extended to infinity, I will just refuse to extend things to infinity. I can always just keep World A with its 5 billion extremely happy people! I like that one! ([View Highlight](https://instapaper.com/read/1532400937/20501729))
